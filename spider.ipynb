{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining CA\n",
    "#### This Text mining assignment will focus on job market mining and follow the general steps follows:\n",
    "- Data Extraction through web crawler.\n",
    "- Text preprocessing.\n",
    "- Use the text mining basic methods list the conclusions.\n",
    "- Advanced methods to get the findings (word2vec). \n",
    " \n",
    "In this part, I mainly wrote the web crawler to finish the data extraction, also tried to refine the data for the raw text data.\n",
    "\n",
    "From https://www.mycareersfuture.sg/, near 220 job results (the website is d) with query about \"machine learning\" are listed in this sample containing the 12 columns. \n",
    "\n",
    "Use the <u>BeautifulSoup</u>, <u>selenium</u> to crawler the data in the web page, and also fetch the json data through request apis.\n",
    "\n",
    "This notebook can be a guideline to show the basic steps.\n",
    "\n",
    "**Attention!!! Download chromedriver in Method 1, and replace the file path in google_chrome_driver_path in Method 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Use the root url like \"https://www.mycareersfuture.sg\" to set the query value to get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to download the driver support in selenium, refer to below 2 helpers.\n",
    "# https://selenium-python.readthedocs.io/installation.html#drivers\n",
    "# https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "google_chrome_driver_path = './chromedriver'\n",
    "root_url = 'https://www.mycareersfuture.sg'\n",
    "\n",
    "driver=webdriver.Chrome(google_chrome_driver_path)\n",
    "driver.get(root_url)\n",
    "# to wait the page finish loading.\n",
    "time.sleep(2) \n",
    "# find and type in the search bar with \"machine learning\"\n",
    "driver.find_element_by_name('search-text').send_keys('machine learning') \n",
    "time.sleep(1) \n",
    "# find and click the search button\n",
    "driver.find_element_by_id('search-button').click()\n",
    "time.sleep(1) \n",
    "# now, get the html of all the search result\n",
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Can just joint the content you wanna query fill in the url:\n",
    "see the url like https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=0\n",
    "- set the *search=* equals to your wanted content such as 'machine%20learning', \"%20\" means the 'space' which is the URL encoding rule, refer to https://zh.wikipedia.org/wiki/%E7%99%BE%E5%88%86%E5%8F%B7%E7%BC%96%E7%A0%81 \n",
    "- set the *page=* equals to the web pages you wanna jump to. attention the **no result**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_url = 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page={}'\n",
    "urls = []\n",
    "# construct 20 pages\n",
    "for page in range(0, 20):\n",
    "    query_url = basic_url.format(page)\n",
    "    urls.append(query_url)\n",
    "# driver.get(urls[0])\n",
    "# html = driver.page_source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start to extract the contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. define the fetch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url, head, payload):\n",
    "    response = requests.get(url, headers=head, params=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {'info': 'error', 'error_code': response.status_code}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. get the query result number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_chrome_driver_path = '/Users/alexjzy/Desktop/Py-Projects/text_mining/chromedriver'\n",
    "driver=webdriver.Chrome(google_chrome_driver_path)\n",
    "query_url = 'https://api.mycareersfuture.sg/jobs?search=machine%20learning&sortBy=new_posting_date'\n",
    "response = fetch_data(query_url, {}, {})\n",
    "result_num = math.ceil(response['count']/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Since the results showing in the pages, in this part get the total number and the total page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "urls shape:  11\n"
     ]
    }
   ],
   "source": [
    "basic_url = 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page={}'\n",
    "urls = []\n",
    "# construct 20 pages\n",
    "for page in range(0, result_num ):\n",
    "    query_url = basic_url.format(page)\n",
    "    urls.append(query_url)\n",
    "print(result_num)\n",
    "print(\"urls shape: \", len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url = 'https://api.mycareersfuture.sg/jobs?search=machine%20learning&sortBy=new_posting_date'\n",
    "response = fetch_data(query_url, {}, {})\n",
    "result_num = math.ceil(response['count']/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=0',\n",
       " 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=1',\n",
       " 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=2',\n",
       " 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=3',\n",
       " 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=4',\n",
       " 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=5',\n",
       " 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=6',\n",
       " 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=7',\n",
       " 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=8',\n",
       " 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=9',\n",
       " 'https://www.mycareersfuture.sg/search?search=machine%20learning&sortBy=new_posting_date&page=10']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls # all the result in pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use the uuid in every card to get the detail of the job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_description(uuid):\n",
    "    api_basic = 'https://api.mycareersfuture.sg/job/{}'\n",
    "    api_jd_url = api_basic.format(uuid)\n",
    "    json = fetch_data(api_jd_url, {}, {})\n",
    "    jd = BeautifulSoup(json['job_description']).get_text(strip=True)\n",
    "    jr = BeautifulSoup(str(json['other_requirements'])).get_text(strip=True)\n",
    "    jsk = [item['skill'] for item in json['skills']]\n",
    "    sal_max = json['max_monthly_salary']\n",
    "    sal_min = json['min_monthly_salary']\n",
    "    return jd, jr, jsk, sal_max, sal_min\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. wrap the json return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detail(card):\n",
    "    company = card.find(\"p\", {\"name\": \"company\"}).get_text()\n",
    "    job_title = card.find(\"h1\", {\"name\": \"job_title\"}).get_text()\n",
    "    \n",
    "    # extract the data\n",
    "    location = card.find_all(\"p\", {\"name\": \"location\"})[0].get_text() if len(card.find_all(\"p\", {\"name\": \"location\"})) > 0 else None\n",
    "    employment_type = card.find_all(\"p\", {\"name\": \"employment_type\"})[0].get_text() if len(card.find_all(\"p\", {\"name\": \"employment_type\"})) > 0 else None\n",
    "    seniority = card.find_all(\"p\", {\"name\": \"seniority\"})[0].get_text() if len(card.find_all(\"p\", {\"name\": \"seniority\"})) > 0 else None\n",
    "    category = card.find_all(\"p\", {\"name\": \"category\"})[0].get_text() if len(card.find_all(\"p\", {\"name\": \"category\"})) > 0 else None\n",
    "    \n",
    "    # get the job detail and collect the jd and requirements which are the raw text\n",
    "    job_uuid = card.find(\"a\", href=True)['href'].split('-')[-1]\n",
    "    job_description, job_requirement, job_skills, salary_max, salary_min = get_job_description(job_uuid)\n",
    "    return {\n",
    "        \"company\": company,\n",
    "        \"job_title\": job_title,\n",
    "        \"location\": location,\n",
    "        \"employment_type\": employment_type,\n",
    "        \"seniority\": seniority,\n",
    "        \"category\": category,\n",
    "        \"job_description\": job_description,\n",
    "        \"job_requirement\": job_requirement,\n",
    "        \"job_skills\": job_skills,\n",
    "        \"job_uuid\": job_uuid,\n",
    "        \"salary_min\": salary_min,\n",
    "        \"salary_max\": salary_max\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. iterate the card in the cards list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_card_info(page_url, res):\n",
    "    driver.get(page_url)\n",
    "    time.sleep(2)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    card_jobs = soup.find(\"div\", {\"class\": \"card-list\"})\n",
    "    cards = card_jobs.find_all(\"div\", {\"class\": \"card relative\"})\n",
    "    for card in cards:\n",
    "        res.append(get_detail(card))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. get the result and convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for url in urls:\n",
    "    get_card_info(url, result)\n",
    "career_res = pd.DataFrame.from_dict(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start to refine and modify the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_res['job_skills'] = career_res.job_skills.apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_res[\"job_description\"] = \\\n",
    "career_res[\"job_description\"].apply(lambda jd:BeautifulSoup(jd).get_text(strip=True))\n",
    "\n",
    "career_res[\"job_requirement\"] = \\\n",
    "career_res[\"job_requirement\"].apply(lambda x:BeautifulSoup(str(x)).get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_res[\"category\"] = career_res.category.apply(lambda x:','.join(x.split('/ ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_res[\"employment_type\"] = career_res.employment_type.apply(lambda x: x.split('...')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "career_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_res.to_csv('mycareersfuture.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
